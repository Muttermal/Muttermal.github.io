---
title: "多模态论文笔记" 
date: 2024-05-14T16:28:21+08:00 
lastmod: 2024-05-15T15:04:21+08:00 
author: ["张广益"] 
tags: ["多模态"]
description: "" #描述
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
math: true	# 显示公式
markup: mmark
draft: false # 是否为草稿
comments: false #是否展示评论
showToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: false # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
  image: "" #图片路径
  caption: "" #图片描述
  alt: ""
  relative: false
---



# 多模态论文笔记

### OneLLM: One Framework to Align All Modalities with Language

[论文地址](https://arxiv.org/pdf/2312.03700.pdf)

**构造一个通用的tokenizer和Project Module 来融合不同模态的信息，解决一般VLM每个模态需要自己的encoder和decoder的痛点。**

**多模态的对齐：**

- 判别对齐 discriminative alignment：CLIP
- 生成对齐 generative alignment： BLIP2

**![OneLLM](OneLLM.png)**

**论文中Universal Encoder采用的是CLIP-ViT，LLM使用的是LLamA-7B**

**改进点：**

- 增加一个可学习的Modality 的token，以便在不同模态间切换；
- 使用export projection结构，这样增加新的模态时可以通过增加export proj来使模型学信新知识，论文中使用的是3个export proj；
- 通过动态路由来决定每个export proj的权重，包括常量router(1/k)、最大值router(直接取最大的)和软router，其中软router效果最好

**缺点：**

- 只能生成文本输出;
- 对于高分辨率的图片输入，无法获取其细节信息；
- 缺乏多模态的高质量数据集

**其他：**

- 训练时冻结LLM模块，可以增加下游图片分类任务的效果；
- 为防止模型灾难性遗忘，可以从LLM或ViT的预训练数据中采样部分数据加入MLLM模型的训练数据中；
- 不同模态联合训练的MLLM效果要好于单独模态训练再拼接后的MLLM；
- 图片文本对齐很有必要，有助于模型对于其他模态的对齐；
- CLIP-ViT作为图像encoder的效果要好于DINOv2，因为后者缺乏图像与文本之间的对齐

### Ferret: Refer and Ground Anything Anywhere at Any Granularity

[论文地址](https://arxiv.org/abs/2310.07704)

**Apple开源的多模态模型，支持多种格式的图像prompt输入（point, box, free-form shapes），同时模型有很强的空间理解和open-vocabulary capabilities。**

![Ferret](Ferret.png)

**主要贡献**

- 模型结构方面，使用了**spatial-aware visual sampler**，基于网格的特征处理手段（如卷积、patch attention）对于不规则的形状区域的效果很差
- 训练数据方面，制作了一个包含 110 万个样本的参考和参考指令调整数据集(GRIT)：
- 模型评测方面，增加了三个空间能力方面的评测任务：引用描述、引用推理和对话基础（Referring Description, Referring Reasoning, and Grounding in Conversation）

**模型结构**

- input：<region_name><coordinates><SPE>text
- spatial-aware visual sampler：给定image feature和region mask，先从mask中随机采样获取N个点，通过线性插值获取点特征，然后这些点特征再经过多个blocks，每个block处理过程如下：
  - sampling：通过FPS算法采样；
  - gathering：对于采样后的点，找到其最相近的k个neighbors组成一组group，然后通过PointMLP处理后得到新的特征；
  - pooling：将每组的k个point通过max pooling融合成一个特征；
- 根据以上处理后，再将得到的point feature拉伸并送到LLM中；
- output：region 坐标 + 相应的文本描述
- 论文中使用的CLIP-ViT-L/14+ LLaMA+LLaVA's projection来初始化

**数据构造**

- 开源数据集，通过SAM整理成特定格式（object detection datasets + visual grounding datasets 总共678k）
- 通过GPT4生成的指令数据集（总共34k）
- 空间数据负采样（spatial negative mining，总共695k）
  - Image-conditioned Category Localization
  - Semantics-conditioned Category Localization

粒度方面包含四种：单个对象；多对象之间的关系；特定区域的描述；基于区域的复杂推理

任务方面分为三种：region-in text-out; text-in region-out; text-region combined data

![Ferrat_data](Ferret_data.png)

**模型评估**

- Referring Object Classification：Is the object ⟨location⟩ a ⟨class A⟩ or a⟨class B⟩?
- Visual grounding：预测边界框；边界框与text对应，What are the locations of <query>/<phrases>?
- Grounded captioning

另外论文中提出一种新的评估方法：**FERRET-BENCH**。它主要从三方面进行评估：

- Referring Description:：要求模型根据参考区域与周围物体的相互作用来描述参考区域；
- Referring Reasoning： 模型需要正确地在一个或多个参考区域之上进行推理；
- Grounding in Conversation：需要模型正确推理并准确地建立/定位推理所需的对象/区域。

利用GPT-4对模型的答案进行打分。

**消融实验**

![Ferret result](Ferret_res.png)

备注：

1. Farthest Point Sampling (FPS) 算法是一种用于从大规模点云数据中选择最远点的算法。这个算法最早被用于计算机图形学中的渲染技术，后来被应用在深度学习领域中的点云处理任务中。

   FPS 算法的核心思想是从点云数据中选择一组点，这些点之间的距离尽可能远。这种采样方法可以帮助减少点云数据的规模，同时保留了原始数据中的重要特征。在深度学习中，FPS 算法通常用于图像识别、三维重建和点云分割等任务中。

   FPS 算法的实现通常包括以下步骤：

   1. 计算点云中每个点与其他点之间的距离。
   2. 选择一个起始点作为第一个采样点。
   3. 从剩余的点中选择与已选取的点距离最远的点作为下一个采样点。
   4. 重复上述步骤，直到达到所需的采样点数量或者比例。

   在深度学习框架中，比如 PyTorch，FPS 算法通常作为一个函数或者模块提供。用户可以通过调用这个函数来实现点云数据的采样。在 PyTorch 中，FPS 算法通常包括 CPU 版本和 GPU 版本，用户可以根据自己的需求选择相应的版本。

   FPS 算法的应用领域非常广泛，包括计算机图形学、深度学习、三维重建等。通过使用 FPS 算法，可以高效地从大规模点云数据中提取关键信息，为后续的数据处理和分析提供有力支持。

2. PointMLP是一种用于点云分析的深度学习网络模型。它是由一组研究人员在ICLR 2022上提出的，旨在解决点云数据结构的不规则性和无序性所带来的挑战。PointMLP的设计旨在提高推理速度并在多个数据集上取得最新的性能表现。

   PointMLP的设计理念是通过使用纯残差MLP（多层感知器）网络，而不是复杂的局部几何提取器，来捕获3D几何信息。此外，PointMLP还配备了一种轻量级的几何仿射模块，用于稳定训练过程。这种设计使得PointMLP在多个数据集上取得了最新的性能表现，并且在推理速度上也表现出色。

   具体来说，PointMLP在真实世界的ScanObjectNN数据集上，甚至比先前最佳方法的准确率高出3.3%。相比于最近的CurveNet模型，PointMLP的训练速度提高了2倍，测试速度提高了7倍，并且在ModelNet40基准测试上更加准确。

   此外，PointMLP的代码已经在GitHub上公开发布，供研究人员和开发者使用。这使得PointMLP成为了一个有潜力帮助研究社区更好地理解点云分析的工具。

   总的来说，PointMLP是一种旨在提高点云分析性能并优化推理速度的深度学习网络模型，它在ICLR 2022上展示了令人满意的性能表现，并且已经在GitHub上开源。

3. **看了别人的工作，感慨我们的工作数据量太少，数据格式太单一，训练LLM的本质还是数据工程**

### LISA: Reasoning Segmentation via Large Language Model

[论文地址](https://arxiv.org/pdf/2308.00692.pdf)

**具备推理分割能力，可以生成mask的多模态模型，LISA在仅在无推理数据集上训练时表现出强大的零样本能力，并且在仅对239对涉及推理的图像-指令对进行微调后，性能得到了显著提升。**

**主要贡献**

- 提出推理分割任务；
- 推理分割任务benchmark, **ReasonSeg**
- 多模态模型LISA

**推理分割任务**

- 与普通分割任务的区别是模型需要具备：复杂的推理、世界知识、解释性答案、多轮对话四种能力

**ReasonSeg**

一个评估数据集，其文本指令主要分为：1.短语级别的；2.长句子级别的。train、val、test的数量分别为239200、1218、779。

**LISA**

![img](lisa_1.png)



作者在词表中新增了一个<SEG>token，如果输出中包含这个token，则通过LLM最后一层的embedding和mask decoder生成mask


$$
\mathbf{h}_{seg} = \gamma(\hat{h}), \quad \mathbf{f}=\mathcal{F}_{enc}(\mathbf{x}_{img}), \quad \hat{\mathbf{M}} = \mathcal{F}_{dec}(\mathbf{h}_{seg}, \mathbf{f})
$$


Loss分为两部分：text Loss(cross entropy) + mask Loss(BCE + DICE)
$$
\mathcal{L} = \lambda_{txt} \mathcal{L}_{txt}  + \lambda_{mask} \mathcal{L}_{mask}
$$

$$
\mathcal{L}_{txt}  =  \mathbf{CE} (\hat{y}_{txt}, y_{txt}),\quad \mathcal{L}_{mask}  =  \lambda_{bce} \mathbf{BCE} (\mathcal{\hat{M}}, \mathcal{M} ) + \lambda_{dice} \mathbf{DICE}(\mathcal{\hat{M}}, \mathcal{M})
$$



训练数据分为三类：

- Semantic Segmentation Dataset：USER: <IMAGE> Can you segment the {CLASS NAME} in this image? ASSISTANT: It is <SEG>.
- Vanilla Referring Segmentation Dataset：USER: <IMAGE> Can you segment {description} in this image? ASSISTANT: Sure, it is <SEG>.

- VQA data:：LaVA-Instruct-150k data 

![img](lisa_2.png)

数据主要来源于开源数据集：ADE20K、COCO-Stuff、PACO-LVIS、refCLEF、refCOCO、refCOCO+等

评估标准：gIoU、cIoU

模型效果：

![img](lisa_3.png)

**消融实验结论**

- vision backbone的选择是多样性的，不限于SAM；
- 对SAM进行微调会影响它的泛化性；
- 语义分割数据为训练提供了大量的二进制mask，所以对模型能力有很大帮助；
- 利用GPT3.5改写文本指令，对模型效果有帮助，可以视为做了数据增强

