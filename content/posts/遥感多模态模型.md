---
title: "遥感多模态模型" 
date: 2024-04-26T16:45:12+08:00 
lastmod: 2024-05-14T16:09:12+08:00 
author: ["张广益"] 
tags: ["多模态", "LMM"]
description: "记录训练遥感多模态模型的完整过程、遗留的问题和一些优化方向"
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
math: true	# 显示公式
draft: false # 是否为草稿
comments: false #是否展示评论
showToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: false # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
  image: "" #图片路径
  caption: "" #图片描述
  alt: ""
  relative: false
---



# 遥感多模态模型

## 背景&需求

随着大模型的迅速发展，公司的一些业务也在探索如何向大模型靠拢。近期我们训练了遥感影像的多模态模型，主要用于影像的解译和基本信息的问答，最终效果不是很理想。本文主要是对整个训练过程中的一些总结和反思。

## 技术路线

### 数据

整个训练过程中，最难的部分就是数据集的制作，最终效果的好坏也取决于训练数据的质量。

我们最开始的目标是训练出**四波段**(RGB加上红外波段，红外波段有助于农作物和水域的识别)的多模态模型，然而考虑到四波段的训练数据过少，以及几乎没有类似于CLIP这样的开源的四波段视觉模型，最终还是决定使用三波段的数据来训练模型。我们的训练数据主要分为两部分：

- 图像文本对数据
- 指令数据集数据

#### 图像文本对

我们收集了一些遥感方面开源的caption数据集，主要信息如下：

- [NWPU](https://github.com/HaiyanHuang98/NWPU-Captions)
- [RSICD](https://github.com/201528014227051/RSICD_optimal)
- [RSITMD](https://github.com/xiaoyuan1996/AMFMN)
- [Sydney_caption](https://pan.baidu.com/s/1hujEmcG#list/path=%2F)
- [UCM_caption](https://pan.baidu.com/s/1mjPToHq#list/path=%2F)
- [RSVG](https://github.com/ZhanYang-nwpu/RSVG-pytorch)

这些数据都是三波段(RGB)的遥感数据集，对应的文本描述为英文描述，这部分数据只有29w左右。我们还收集了一些分类任务的遥感数据集，并用开源的多模态模型来生成文本描述，这部分数据有30w左右。此外我们还使用了部分开源的通用领域中文数据集，后来分析结果时发现这部分数据对模型的影响较大，不应该加入到训练数据中，原因放在总结部分讨论。

最终图像文本对的数据量为82w。

#### 指令数据集

指令数据集方面，我们收集了一些遥感领域的开源指令数据集，来源如下：

- [rsvqa](https://rsvqa.sylvainlobry.com/)
- [rsivqa](https://github.com/spectralpublic/RSIVQA)
- [mqvqa](https://github.com/MeimeiZhang-data/MQVQA)
- [geo_chat](https://huggingface.co/datasets/MBZUAI/GeoChat_Instruct)

对这部分数据，我们进行了一些去重和过滤处理，如去掉一些人工难以识别的指令（面积、数量等），对不用数据集中出现的语义标签进行了统一处理，以及统一了所有数据集的格式。

除了上述数据，我们还从语义分割数据集中制作了部分指令数据集。我们构造了一些基础指令集，主要包括：指定位置分割、指定标签分割、选项选择、数量统计等，示例如下：

> Q: "图中有哪些语义？"
>
> A: "图中包含{labels}"
>
> 
>
> Q: "图中包含{label}吗？"
>
> A: "是的，图中包含{label}"
>
> A: "不，图中不包含{neg_label}，但包含{true_label}"
>
> 
>
> Q: "图中包含什么语义，请从以下选项中选出合适的答案: {options}"
>
> A: "答案是{answer}，图中有{labels}"
>
> 
>
> Q: "分割出{label}的位置"
>
> A: "{label}的位置是{box}"
>
> Q: "图中{box}处是什么？"
>
> A: "是{label}"

构造基础指令集后，为了丰富对话的内容，我们使用开源的LLM对单一的对话进行改造，如:

```python
qa_rich_prompt = """作为一个人工智能视觉助手，您现在面临一个问题，该问题与一张图片相关，并附有一个简短的标准答案。您的任务是将这个标准答案转化为自然且有说服力的回答。确保回答准确无误，高度贴合问题，并与原答案保持一致。 
问题： 
图中篮球场的旁边是什么？ 
答案： 
足球场 
转化后的答案： 
篮球场的旁边是一个足球场。
问题： {Q} 
答案： {A}
转化后的答案：
"""

raw_content = {"Q": "图中有几座桥", "A": "3"}
# 模型丰富后的对话
new_content = {"Q": "图中有几座桥", "A": "在图片中，你可以清晰地看到有三座桥。"}
```

最终数据集中语义分割的指令较多，我们采用的范围框的格式为{x,y,h,w,$\theta$}，$\theta$​为旋转角。在构造范围框时，我们对不同分辨率的图片进行了归一化，最终将所有的坐标值限定在0到1000内。

此外，我们还增加了部分纯文本的指令数据，主要是[COIG-CQIA](https://huggingface.co/datasets/m-a-p/COIG-CQIA)和[RefGPT-Fact](https://huggingface.co/datasets/Mutonix/RefGPT-Fact)这两个中文数据集。纯文本数据有助于防止模型的纯文本能力退化，同时可以提高VAQ任务的准确率。

最后，我们还加入了一些通用的图片指令数据来扩充指令的多样性，从结果来看，这部分数据并未增强模型对遥感图片的识别能力，原因还是在于通用图片和遥感图片的巨大差异性。

最终我们使用的指令数据集数量为92w。

### 模型

我们最初调研了一些偏CV领域的分割模型，如[SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)、[SegGPT](https://github.com/baaivision/painter)等。这些模型并没有用到LLM，而是更偏SAM架构的纯分割模型，他们的优点如下：

- 训练路线较易实现，需要的训练资源相对较少
- 比较契合公司目前已有的数据结构
- 对语义分割类的任务效果较好

然而这类模型并不算真正的多模态模型，且只针对于语义分割这个任务。考虑到后续这个模型还需要执行其他的任务（如变化检测、影像质检），我们采用了[LLaVA](https://github.com/haotian-liu/LLaVA)的技术方案，整个模型的结构分为三部分：

1. Image Encoder；
2. Projector，主要作用是将图像的语义空间映射到文本的语义空间；
3. LLM，大模型部分。

对于Image Encoder，我们使用的是在[遥感数据集RS5M](https://huggingface.co/datasets/Zilun/RS5M)上预训练过的一个CLIP模型[GeoRSCLIP](https://huggingface.co/Zilun/GeoRSCLIP)；Projector部分使用的是2层的MLP；LLM使用的是Qwen1.5-14b。

由于最终的需求是生成语义标签的掩膜，而多模态模型只能生成范围框，所以我们在多模态模型后面又增加了一个DinoV2模型，用来将多模态模型生成的范围框转变为更加精细的掩膜文件。这个模型是使用公司内部数据训练而来。

### 训练&评估

训练过程主要分为两步：

1. 使用图像文本对数据的预训练，训练时冻结Image Encoder部分和LLM部分，主要训练Projector部分；
2. 使用指令数据集数据的微调，训练时冻结Image Encoder部分和Projector部分，微调LLM部分。

训练的代码主要参考自[LLavA](https://github.com/haotian-liu/LLaVA)的代码，在上面做了一些改动。整个训练过程中使用的是一台A40 48G * 8的服务器，第一阶段总batch size 为256，训练时长为30小时，第二阶段使用LoRA来进行微调，总batch size 为128，训练时长为58小时。

训练完成后，人工测试评估发现效果不太理想，主要的问题如下：

1. 回答中有部分重复的内容；
2. 对于图像的描述，模型会出现幻觉，生成一些图片中不存在的描述；
3. 在通用图片上的效果明显好于遥感图片；
4. 范围框的生成不是很精确，会出现遗漏和超出的现象

## 总结&反思

什么样的数据训练出什么样的模型。对于最终不如人意的结果，我觉得原因如下：

- 遥感图片和通用图片的差异：遥感影像分为航拍影像和卫星影像，这两种都是正射影像（从正上方往下拍），而通用图片包含各种各样的视角。前者通常需要从全局的角度去判断语义，后者则是从细节去判断语义。我们在训练集中加入了通用的数据，且数据量比遥感图片数据还要多，这就导致了模型对于图片的理解更偏向于细节，而缺失了对于当前图片整体的理解。
- 我们的数据大多来自开源数据集和人工构造：对于图像描述数据，有一部分数据来自模型生成，这部分的数据存在幻觉问题，生成的内容常常会包含一些图片中不存在的信息；对于指令数据集，遥感相关的指令和回复基本是靠模板来构造的，这就导致指令的回复通常很相似，或者很简短，最终导致模型的幻觉问题和生成回复的重复性。**指令的多样性和回复的丰富程度，很大程度上决定了模型的生成质量**。

对于模型结构，我们还参考过[LISA](https://github.com/dvlab-research/LISA?tab=readme-ov-file)，然而LISA采用了SAM的mask decoder来生成掩膜，这种方式无法扩充到四波段的影像。随着开源社区多模态模型越来越多，也有很多工作提出了新的观点。如[Ferret2](https://arxiv.org/abs/2404.07973)使用了两个Image Encoder：CLIP和DinoV2，前者用来提取图片的全局信息，后者用来提取局部信息；[LLaVA-Next](https://llava-vl.github.io/blog/2024-01-30-llava-next/)将图片resize成四张图后再拼上原图，将五张图片encoding后送进LLM中，以提高模型对于图片的理解能力；[Mini-Gemini](https://github.com/dvlab-research/MGM)将图片下采样后得到低清晰度样本，将原图（高清样本）和低清样本做cross-attention后再送进LLM中。这些策略都有助于模型从整体和局部的角度更好的理解图片内容。

对于训练数据，除了图像文本对数据和指令数据集外，[VILA](https://arxiv.org/pdf/2312.07533.pdf)中指出加入交错的图像文本数据可以提高模型效果。

近期HuggingFace团队放出了他们最新的多模态模型[Idefics2](https://huggingface.co/HuggingFaceM4/idefics2-8b)，他们的[技术报告](https://arxiv.org/pdf/2405.02246)中也探索了哪些因素对模型是有帮助的。如Image Encoder模型的选择、数据集的构造等，这些方法和结论也是我们日后优化模型的方向。
