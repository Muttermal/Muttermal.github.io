

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/new_icon.png">
  <link rel="icon" href="/img/new_icon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhang Guangyi">
  <meta name="keywords" content="carpe diem">
  
    <meta name="description" content="本文介绍了对抗训练的基本思路和相关模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="对抗训练简介">
<meta property="og:url" content="https://muttermal.github.io/2022/06/23/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E7%AE%80%E4%BB%8B/index.html">
<meta property="og:site_name" content="Muttermal&#39;s Blog">
<meta property="og:description" content="本文介绍了对抗训练的基本思路和相关模型。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://muttermal.github.io/article_banner/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83.jpg">
<meta property="article:published_time" content="2022-06-23T02:00:44.000Z">
<meta property="article:modified_time" content="2022-06-24T03:55:00.000Z">
<meta property="article:author" content="Zhang Guangyi">
<meta property="article:tag" content="对抗训练">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://muttermal.github.io/article_banner/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83.jpg">
  
  
  
  <title>对抗训练简介 - Muttermal&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/prism/1.27.0/plugins/line-numbers/prism-line-numbers.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"muttermal.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"g0JwxKgWMFt1i2ALIuDyJvfF-MdYXbMMI","app_key":"oJEaE0K8zW8jdDT4mJbAb6TD","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Muttermal</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner_picture.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="对抗训练简介"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-06-23 10:00" pubdate>
          2022年6月23日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          61 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">对抗训练简介</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2022年6月24日 中午
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="对抗训练简介">对抗训练简介</h1>
<p align="center">
<a><img src="https://img.shields.io/badge/Muttermal-对抗训练-ff69b4" srcset="/img/loading.gif" lazyload></a>
</p>
<h2 id="发展背景">1.发展背景</h2>
<p>​ 对抗训练（Adversarial Training）最初由 GAN之父Ian
Goodfellow等人提出<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)">[1]</span></a></sup>，作为一种防御对抗攻击的方法，最早应用在计算机视觉领域，用来解决利用对抗样本来恶意攻击模型的问题。</p>
<h2 id="基本思路">2.基本思路</h2>
<p>​ 我们定义<span class="math inline">\(D\)</span>为训练集，<span class="math inline">\(x\)</span>代表输入，<span class="math inline">\(y\)</span>代表标签，<span class="math inline">\(\theta\)</span>代表模型参数，<span class="math inline">\(L\)</span>为损失函数，<span class="math inline">\(r_{adv}\)</span>为扰动。对抗训练的思路就是在原始输入样本
<span class="math inline">\(x\)</span>上增加一个扰动 <span class="math inline">\(r_{adv}\)</span>，得到<strong>对抗样本</strong>后，用其进行训练。我们可以把问题抽象成如下模型：
<span class="math display">\[
\min \limits_{\theta} -\log P(y|x+r_{adv};\theta)
\]</span> 从模型优化的角度，我们可以将对抗训练拆解为两部分：</p>
<p>(1).从扰动的范围空间<span class="math inline">\(S\)</span>内找到对模型最有效的扰动，这部分可看作是对模型的攻击：
<span class="math display">\[
\max \limits_{r_{adv}\in S}L(\theta, x+r_{adv}, y)
\]</span> (2).找到最鲁棒的模型参数<span class="math inline">\(\theta\)</span>使得对抗样本对模型的影响降到最低，也即模型的防御：
<span class="math display">\[
\min\limits_{\theta}E_{(x,y)\sim D}[\max\limits_{r_{adv}\in S}L(\theta,
x+r_{adv}, y)]
\]</span>
上式就是对抗训练中最有名的Min-Max公式，它可以看成内部损失函数最大化+外部经验风险最小化的结合。</p>
<p>有了上述的分析，接下来的问题就在于：如何构造足够强的对抗样本？如何让模型具备最高的防御能力？</p>
<h3 id="对抗样本">对抗样本</h3>
<p>​ 在原论文中，作者提出了<strong>Fast Gradient Sign
Method(FGSM)</strong>来构造输入样本的扰动。扰动的定义如下： <span class="math display">\[
r_{adv} = \epsilon * sgn(\nabla_{x}L(\theta,x,y))
\]</span> 其中<span class="math inline">\(sgn\)</span>为符号函数。</p>
<figure>
<img src="/2022/06/23/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E7%AE%80%E4%BB%8B/adv_sample.png" srcset="/img/loading.gif" lazyload alt="熊猫加扰动后变成了长臂猿">
<figcaption aria-hidden="true">熊猫加扰动后变成了长臂猿</figcaption>
</figure>
<p>​</p>
<p>从FGSM的设计中我们也可以总结出对抗样本需要具备的两个特点：</p>
<p>​ 1.添加的扰动需要是微小的，对抗样本与原始样本之间需要满足约束<span class="math inline">\(\left\| x^* - x \right\|_{\infty} =\left\|
r_{adv}\right\|_{\infty}\leq \epsilon\)</span>；</p>
<p>​ 2.应该从Loss梯度上升的方向来添加扰动，从而尽量使模型出错。</p>
<h2 id="主要方法">3.主要方法</h2>
<p>​
上述提到的思路都是基于CV领域提出的，CV的输入是连续的RGB的值，根据上述方法算出扰动后可以直接加到输入上来构造对抗样本。对于NLP领域来说，有没有办法将这种思路迁移过来呢？答案肯定是有的。NLP的输入一般为one-hot向量，直接在上面加扰动肯定是不行的，Goodfellow在文章<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Adversarial Training Methods for Semi-Supervised Text Classification](https://arxiv.org/abs/1605.07725)">[2]</span></a></sup>中提出可以在连续的embedding上做扰动。这种方法相较于CV来说最大的不同是，CV的对抗样本可以还原成图像，但是NLP的对抗样本无法还原成某个单词或词组。<strong>所以在NLP任务中，对抗训练的目的不再是为了防御基于梯度的恶意攻击，更多是作为一种正则化手段来提高模型的泛化能力。</strong></p>
<p>NLP中常见的对抗训练有以下几种：</p>
<h3 id="fast-gradient-methodfgm">3.1 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1605.07725.pdf">Fast Gradient
Method(FGM)</a></h3>
<p>​
FGM在上文提到的FGSM上做了一点改动，将原本的符号函数变成了映射到单位球面上的向量：
<span class="math display">\[
g = \nabla_{x}L(\theta,x,y), \quad r_{adv} = \epsilon * g / \left\| g
\right\|_2
\]</span> 上式中<span class="math inline">\(x\)</span>为embedding向量，主要代码如下<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="代码参考自：[【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现](https://zhuanlan.zhihu.com/p/91269728)">[4]</span></a></sup>：</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""
对于每个x:
  1.计算x的前向loss、反向传播得到梯度
  2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r
  3.计算x+r的前向loss，反向传播得到对抗的梯度，累加到(1)的梯度上
  4.将embedding恢复为(1)时的值
  5.根据(3)的梯度对参数进行更新
"""</span>
<span class="token keyword">class</span> <span class="token class-name">FGM</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model
        self<span class="token punctuation">.</span>backup <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>

    <span class="token keyword">def</span> <span class="token function">attack</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1.</span><span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'emb.'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 增加扰动</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
                norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
                <span class="token keyword">if</span> norm <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                   	<span class="token comment"># 一个batch内的样本统一用一个范数归一化</span>
                    r_at <span class="token operator">=</span> epsilon <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> norm
                    param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>r_at<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">restore</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'emb.'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 还原embedding向量</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span> 
                <span class="token keyword">assert</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>backup
                param<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>backup <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure>
<p>原论文的代码中，是对一个batch内的每个样本分别求范数。</p>
<h3 id="projected-gradient-descentpgd">3.2 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.06083.pdf">Projected Gradient
Descent(PGD)</a></h3>
<p>​
<strong>PGD</strong>从<strong>FGM</strong>的一步到位改成了做多次迭代，在寻找最优扰动时，作者采用了<strong>”小步走，多走几步“</strong>的迭代方法，每次走一小步，每次迭代都将扰动映射到约束空间内：
<span class="math display">\[
x_{t+1} = \mathop{\Pi} \limits_{r_{adv}\in S}(x_t + r_{adv})
\]</span></p>
<p><span class="math display">\[
r_{adv}=\alpha*g(x_t)/ \left\| g(x_t)\right\|_2,\quad g =
\nabla_{x}L(\theta,x,y)
\]</span></p>
<p>其中<span class="math inline">\(S=\left\{ r\in R^d: \left\|
r\right\|_2 \leq \epsilon \right\}\)</span>为扰动的约束空间，<span class="math inline">\(\alpha\)</span>为每一步的步长，<span class="math inline">\(\mathop{\Pi} \limits_{r_{adv}\in S}(x_t +
r_{adv})\)</span>表示将<span class="math inline">\((x_t +
r_{adv})\)</span>映射到<span class="math inline">\(S\)</span>上。主要代码如下<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="代码参考自：[【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现](https://zhuanlan.zhihu.com/p/91269728)">[4]</span></a></sup>：</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""
对于每个x:
  1.计算x的前向loss、反向传播得到梯度并备份
  对于每步t:
    2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r(超出范围则投影回S内)
    3.t不是最后一步: 将梯度归0，根据1的x+r计算前后向并得到梯度
    4.t是最后一步: 恢复(1)的梯度，计算最后的x+r并将梯度累加到(1)上
  5.将embedding恢复为(1)时的值
  6.根据(4)的梯度对参数进行更新
"""</span>
<span class="token keyword">class</span> <span class="token class-name">PGD</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model
        self<span class="token punctuation">.</span>steps <span class="token operator">=</span> steps	<span class="token comment"># 迭代步数</span>
        self<span class="token punctuation">.</span>emb_backup <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>grad_backup <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>

    <span class="token keyword">def</span> <span class="token function">attack</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1.</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'emb.'</span><span class="token punctuation">,</span> is_first_attack<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># emb_name为模型中embedding的参数名</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_first_attack<span class="token punctuation">:</span>
                    self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
                norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
                <span class="token keyword">if</span> norm <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    r_at <span class="token operator">=</span> alpha <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> norm
                    param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>r_at<span class="token punctuation">)</span>
                    param<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>project<span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">,</span> epsilon<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">restore</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'emb.'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span> 
                <span class="token keyword">assert</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>emb_backup
                param<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>emb_backup <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        
    <span class="token keyword">def</span> <span class="token function">project</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> param_name<span class="token punctuation">,</span> param_data<span class="token punctuation">,</span> epsilon<span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> param_data <span class="token operator">-</span> self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>param_name<span class="token punctuation">]</span>
        <span class="token comment"># 如果扰动超出约束空间，就映射回去</span>
        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token operator">></span> epsilon<span class="token punctuation">:</span>
            r <span class="token operator">=</span> epsilon <span class="token operator">*</span> r <span class="token operator">/</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>r<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>param_name<span class="token punctuation">]</span> <span class="token operator">+</span> r
        
    <span class="token keyword">def</span> <span class="token function">backup_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>grad_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">restore_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                param<span class="token punctuation">.</span>grad <span class="token operator">=</span> self<span class="token punctuation">.</span>grad_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure>
<p>调用方式：</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">pgd <span class="token operator">=</span> PGD<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch_input<span class="token punctuation">,</span> batch_label <span class="token keyword">in</span> data<span class="token punctuation">:</span>
    <span class="token comment"># 正常训练</span>
    loss <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_input<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播，得到正常的grad</span>
    pgd<span class="token punctuation">.</span>backup_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 对抗训练</span>
    <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>pgd<span class="token punctuation">.</span>steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        pgd<span class="token punctuation">.</span>attack<span class="token punctuation">(</span>is_first_attack<span class="token operator">=</span><span class="token punctuation">(</span>t<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># first attack时备份param.data</span>
        <span class="token keyword">if</span> t <span class="token operator">!=</span> pgd<span class="token punctuation">.</span>steps <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
            model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            pgd<span class="token punctuation">.</span>restore_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss_adv <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_input<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>
        loss_adv<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span>
    pgd<span class="token punctuation">.</span>restore<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 恢复embedding参数</span>
    <span class="token comment"># 梯度下降，更新参数</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure>
<h3 id="free-adversarial-trainingfreeat5">3.3 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.12843.pdf">Free Adversarial
Training(FreeAT)</a><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="[论文阅读：对抗训练(adversarial training)](https://zhuanlan.zhihu.com/p/104040055)">[5]</span></a></sup></h3>
<p>​ 从<strong>FGM</strong>到<strong>PGD</strong>，主要优化了扰动 <span class="math inline">\(r_{adv}\)</span>
的计算，但是计算量也增加了。在<strong>FGM</strong>中，每个样本每次迭代需要计算两次（一次<span class="math inline">\(x\)</span>，一次<span class="math inline">\(x+r_{adv}\)</span>）,而<strong>PGD</strong>中则要计算
<span class="math inline">\(K+1\)</span>
次，消耗了更多的计算资源，而且在梯度下降时只利用了参数的梯度，梯度提升时只利用输入的梯度，有没有办法把计算出来的梯度和输入的梯度同时利用上呢？这就是<strong>FreeAT</strong>的核心思想。</p>
<p><strong>PGD</strong>采用的是对每个样本求K次梯度，K步走完后，才重新计算一次梯度用来更新模型参数，而<strong>FreeAT</strong>则是每走一步求一次梯度，更新一次模型参数，具体差别可以用下图来表示：</p>
<figure>
<img src="/2022/06/23/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E7%AE%80%E4%BB%8B/FreeAT.png" srcset="/img/loading.gif" lazyload alt="FreeAT">
<figcaption aria-hidden="true">FreeAT</figcaption>
</figure>
<p>​
这样一来，<strong>FreeAT</strong>中模型参数更新的次数是普通训练的K倍，所以他把总体的epoch次数除以K来保证梯度计算次数跟普通训练的一样。这样会带来一个问题，一个mini-batch的样本被模型训练了K遍，可能影响到模型收敛的效果（论文中作者用实验结果表示这种担心不太必要）。</p>
<p>实现上，<strong>FreeAT</strong>在计算下一步扰动的梯度时，复用上一步的梯度，r的更新公式为：
<span class="math display">\[
r_{t+1} = r_t + \epsilon * sign(g),\quad g = \nabla_{x}L(\theta,x,y)
\]</span></p>
<p>主要思路如下：</p>
<blockquote>
<p>初始化r=0 对于epoch=1...N/m: 对于每个x: 对于每步m:
1.利用上一步的r，计算x+r的前后向loss，得到梯度 2.根据梯度更新参数
3.根据梯度更新r</p>
</blockquote>
<h3 id="free-large-batchfreelb">3.4 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.11764.pdf">Free
Large-Batch(FreeLB)</a></h3>
<p>​ 可以看到在<strong>PGD</strong>中只使用了最后一步<span class="math inline">\(x+r_{adv}\)</span>输出的梯度，<strong>FreeLB</strong>在这里做了改进，它取了每次迭代更新<span class="math inline">\(r_{adv}\)</span>过程中梯度的平均值，相当于在最大化扰动过程中，把原本的
<span class="math inline">\(\max \limits_{r_{adv}\in S}L(\theta,
x+r_{adv}, y)\)</span> 变成了 <span class="math inline">\(\frac{1}{K}\sum\limits_{t=0}^{K-1}\max
\limits_{r_{t}\in S}L(\theta, x+r_{t}, y)\)</span>，其中 <span class="math inline">\(K\)</span> 为迭代的步数。</p>
<p>代码思路如下：</p>
<blockquote>
<p>对于每个x: 1.通过均匀分布初始化r，梯度g为0 对于每步t=1...K:
2.根据x+r计算前后向，累计梯度g 3.更新r 4.根据g/K更新梯度</p>
</blockquote>
<p>论文中作者还指出<strong>对抗训练和dropout不能同时使用</strong>，dropout相当于改变了网络结构，会影响扰动
<span class="math inline">\(r_{adv}\)</span> 的计算。</p>
<h3 id="smoothness-inducing-adversarial-reregularizationsmart">3.5 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.03437.pdf">SMoothness-inducing
Adversarial ReRegularization(SMART)</a></h3>
<p>​
前面提到的对抗训练方法都是针对添加扰动后传回的梯度进行处理，并没有在损失函数上做改动，而<strong>SMART</strong>则是在Loss上添加了一个正则项：<strong>Smoothness-inducing
Adversarial
Regularization</strong>，目的是在一定的范围内，找到对模型影响最大的扰动<span class="math inline">\(r\)</span>： <span class="math display">\[
\mathcal{R}_s(\theta)=\frac{1}{n}\sum\limits_{i=1}^n \max
\limits_{r_{i}\in S}l_s(f(x_i+r_{i};\theta), f(x_i;\theta))
\]</span> 其中<span class="math inline">\(l_s\)</span>为<strong>对称的KL散度</strong><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="对称KL散度又称JS散度，$JSD(P||Q) = \frac{1}{2}KL(P||M) + \frac{1}{2}KL(Q||M),\quad M=\frac{1}{2}(p+Q)$​，这里与传统的JS散度略有不同.">[3]</span></a></sup></p>
<p><span class="math display">\[
l_s(P,Q) = D_{KL}(P||Q) + D_{KL}(Q||P)
\]</span></p>
<p>模型的优化目标就在于 <span class="math display">\[
\mathop{argmin}_\limits \theta \mathcal{F}(\theta) =
\mathop{argmin}_\limits \theta \bigg(\mathcal{L}(\theta) +
\lambda_s\mathcal{R}_s(\theta)\bigg)
\]</span></p>
<p>为了求解（11）式，作者还提出了<strong>Bregman Proximal Point
Optimization</strong>优化方法，对于第<span class="math inline">\(t+1\)</span>轮迭代： <span class="math display">\[
\theta_{t+1} = \mathop{argmin}\limits_{\theta}\bigg(\mathcal{F}(\theta)
+ \mu\mathcal{D}_{Breg}(\theta,\theta_t)\bigg)
\]</span> 其中<span class="math inline">\(\theta_0\)</span>为预训练模型的参数，<span class="math inline">\(\mathcal{D}_{Breg}(\theta,\theta_t)\)</span>（布雷格曼散度）的定义如下：
<span class="math display">\[
\mathcal{D}_{Breg}(\theta,\theta_t)=\frac{1}{n}\sum\limits_{i=1}^n
l_s(f(x_i;\theta), f(x_i;\theta_t))
\]</span> <span class="math inline">\(\mathcal{D}_{Breg}(\theta,\theta_t)\)</span>也可以看做是一个正则项，用来防止模型参数更新的幅度过大。</p>
<p>在这两项正则项下，模型的最终损失函数可以写成如下形式： <span class="math display">\[
\mathcal{F}(\theta) = \mathcal{L}(\theta) +
\lambda_s\mathcal{R}_s(\theta) +\mu\mathcal{D}_{Breg}(\theta,\theta_t)
\]</span> 最终的优化策略如下：</p>
<blockquote>
<p>对于第t轮迭代：</p>
<p>1.备份模型参数<span class="math inline">\(\theta\)</span>，作为计算<span class="math inline">\(\mathcal{D}_{Breg}(\theta,\theta_t)\)</span>中的<span class="math inline">\(\theta_t\)</span>；</p>
<p>2.对于每个batch内的第 <span class="math inline">\(i\)</span>
个数据：</p>
<p>​ 使用正态分布随机初始化扰动<span class="math inline">\(r_i\)</span>，结合<span class="math inline">\(x_i\)</span>得到对抗样本 <span class="math inline">\(x_i+r_i\)</span>；</p>
<p>​ 对于m个小步：</p>
<p>​ 计算扰动下的梯度<span class="math inline">\(\widetilde
g\)</span>；</p>
<p>​ 基于<span class="math inline">\(\widetilde
g\)</span>和学习率更新对抗样本<span class="math inline">\(x_i+r_i\)</span>；</p>
<p>​ 基于对抗样本<span class="math inline">\(x_i+r_i\)</span>重新计算梯度，更新模型参数<span class="math inline">\(\theta\)</span>；</p>
<p>3.更新<span class="math inline">\(\theta_t,\quad
\theta_t=(1-\beta)\theta + \beta\theta_t\)</span>，<span class="math inline">\(\beta\)</span>为动量参数。</p>
</blockquote>
<p>总体来说<strong>SMART</strong>的思路主要分为两部分：</p>
<p>1.训练过程中加入对embedding的随机扰动，要求模型的输出尽可能与扰动前的概率保持一致；</p>
<p>2.在模型更新参数时，修改优化器（Adam）的结果，要求模型参数与预训练时的参数分布相近，尽可能少改变。</p>
<h2 id="参考资料">参考资料</h2>
<section class="footnotes">
<div class="footnote-list">
<ol>
<li>
<span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6572">Explaining
and Harnessing Adversarial Examples</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref">
↩︎</a></span></span>
</li>
<li>
<span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1605.07725">Adversarial
Training Methods for Semi-Supervised Text Classification</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref">
↩︎</a></span></span>
</li>
<li>
<span id="fn:3" class="footnote-text"><span>对称KL散度又称JS散度，<span class="math inline">(JSD(P||Q) = KL(P||M) + KL(Q||M),
M=(p+Q))</span>​，这里与传统的JS散度略有不同.
<a href="#fnref:3" rev="footnote" class="footnote-backref">
↩︎</a></span></span>
</li>
<li>
<span id="fn:4" class="footnote-text"><span>代码参考自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91269728">【炼丹技巧】功守道：NLP中的对抗训练
+ PyTorch实现</a>
<a href="#fnref:4" rev="footnote" class="footnote-backref">
↩︎</a></span></span>
</li>
<li>
<span id="fn:5" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104040055">论文阅读：对抗训练(adversarial
training)</a>
<a href="#fnref:5" rev="footnote" class="footnote-backref">
↩︎</a></span></span>
</li>
</ol>
</div>
</section>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Deep-Learning/" class="category-chain-item">Deep Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/">#对抗训练</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>对抗训练简介</div>
      <div>https://muttermal.github.io/2022/06/23/对抗训练简介/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Zhang Guangyi</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年6月23日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2022年6月24日</div>
        </div>
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/06/27/CLIP%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/" title="CLIP模型介绍">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CLIP模型介绍</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/06/21/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" title="对比学习总结">
                        <span class="hidden-mobile">对比学习总结</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"Cjw9JJwLdh5c0kbMie1SEiT5-gzGzoHsz","appKey":"OIfEEMJjgRTnzvbYTgnQa1T0","path":"window.location.pathname","placeholder":"说点什么吧！","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script  src="https://lib.baomitu.com/prism/1.27.0/plugins/line-numbers/prism-line-numbers.min.js" ></script>

  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
